{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0. Basic Text Preprocessing.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onlbxqdN_MxE"
      },
      "source": [
        "# Basic Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBrrPHDE_QM3"
      },
      "source": [
        "## Removing extra White Spaces\n",
        "\n",
        "Most of the time the text data that you have may contain extra spaces in between the words, after or before a sentence. So to start with we will remove these extra spaces from each sentence by using regular expressions or by using any basic techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDAcfNWi9fSp"
      },
      "source": [
        "example_text = \"NLP  is an interesting     field.  \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAEoJnu0_fdI",
        "outputId": "812228d9-5c95-4589-9a80-d79ad917b78e"
      },
      "source": [
        "print('\\n')\n",
        "print('Text before removing white spaces : \"{}\"'.format(example_text))\n",
        "split_text = example_text.split(' ')\n",
        "print('\\n')\n",
        "print('Let us split the text: \"{}\"'.format(split_text))\n",
        "print('\\n')\n",
        "print('we see empty spaces in this list let us try to get rid of them!')\n",
        "cleaned_text_tokens = []\n",
        "for i in split_text:\n",
        "  if i=='':\n",
        "    pass\n",
        "  else:\n",
        "    cleaned_text_tokens.append(i)\n",
        "print('\\n')\n",
        "print('List after getting rid of the empty spaces \"{}\"'.format(cleaned_text_tokens))\n",
        "print('\\n')\n",
        "print('This looks good, Now let us join them back using a space!')\n",
        "cleaned_text = ' '.join(cleaned_text_tokens)\n",
        "print('\\n')\n",
        "print('Text after removing white spaces  : \"{}\"'.format(cleaned_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Text before removing white spaces : \"NLP  is an interesting     field.  \"\n",
            "\n",
            "\n",
            "Let us split the text: \"['NLP', '', 'is', 'an', 'interesting', '', '', '', '', 'field.', '', '']\"\n",
            "\n",
            "\n",
            "we see empty spaces in this list let us try to get rid of them!\n",
            "\n",
            "\n",
            "List after getting rid of the empty spaces \"['NLP', 'is', 'an', 'interesting', 'field.']\"\n",
            "\n",
            "\n",
            "This looks good, Now let us join them back using a space!\n",
            "\n",
            "\n",
            "Text after removing white spaces  : \"NLP is an interesting field.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jtNv69sA8_z"
      },
      "source": [
        "- Pretty much clear with what we have done, now lets move on to our next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al9q36t1CM6T"
      },
      "source": [
        "## Removing Punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrNfW4m4CQC6"
      },
      "source": [
        "Getting rid of punctuation is extremely important as they end up being tokens after tokenization of the text which will bring equal importance to it as it is with a normal word. As this text preprocessing often is useful for making TF-IDF models we donot want a punctuation mark to be a token and give its importance to the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDUr1lzq_mkS"
      },
      "source": [
        "# Importing string to print all punctuation marks.\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kniJ1lEQAcBh",
        "outputId": "bde6fa43-8309-43be-cf08-3766a19c2bc8"
      },
      "source": [
        "print(string.punctuation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2-u8dMJDH9S"
      },
      "source": [
        "- There we go here we have all the punctuations.\n",
        "- Now let us take an example and try to remove these punctuations from it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRyPzJZ7DG9C"
      },
      "source": [
        "example_text = \"Hello! How are you!! I'm very excited that you're going for a trip to Europe!! Yayy!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pmNiThvDYdE",
        "outputId": "95262c66-ed4e-4b44-ee61-50250c5db9b2"
      },
      "source": [
        "print('\\n')\n",
        "print('Text before cleaning : \"{}\"'.format(example_text))\n",
        "new_text = ''\n",
        "for i in example_text:# Iterationg through each element of the string so as to get rid of the punctuation marks.\n",
        "  if i in string.punctuation:\n",
        "    pass\n",
        "  else:\n",
        "    new_text=new_text+i \n",
        "print('\\n')\n",
        "print('Text after cleaning : \"{}\"'.format(new_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Text before cleaning : \"Hello! How are you!! I'm very excited that you're going for a trip to Europe!! Yayy!\"\n",
            "\n",
            "\n",
            "Text after cleaning : \"Hello How are you Im very excited that youre going for a trip to Europe Yayy\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nobUO3mf_9lN"
      },
      "source": [
        "- As we are done with the punctuation removal lets just go further.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XpuHglbsxK5"
      },
      "source": [
        "## Lowering words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJlqmIDks11B"
      },
      "source": [
        "- Lower casing all words is very much important or else words like 'Hii' and 'hii' end up being different words where as they pretty much explain the same thing.\n",
        "- We will split the words at spaces after all the removal of punctuations from the texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ygFiWyHwEGzX",
        "outputId": "7e580b95-6a75-4ea7-d329-3f1f2190d332"
      },
      "source": [
        "new_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello How are you Im very excited that youre going for a trip to Europe Yayy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBcreELNtHpQ",
        "outputId": "7cb5d4d8-2c03-4439-efb0-824a00e26797"
      },
      "source": [
        "new_text = new_text.split(' ')\n",
        "new_text = [i.lower() for i in new_text]\n",
        "new_text = ' '.join(new_text)\n",
        "print(new_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello how are you im very excited that youre going for a trip to europe yayy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5FF14npuQXB"
      },
      "source": [
        "- Let us proceed a bit further by removing all those stopwords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YepuQe6NuVBi"
      },
      "source": [
        "## Removing StopWords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40ajNdcktlHy"
      },
      "source": [
        "\n",
        "### Stopwords explain nothing ?\n",
        "- Well they do explain alot but in Classical ML approach the context based models are rarely seen and the solution becomes lot more better as we get rid of those words and try to make our *corpus of words(Bag of Words) as small as possible.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7qacUHxtjg-"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rucV-EB_uhlj",
        "outputId": "3711d125-f242-4556-bc13-53d444db47cf"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAXOjYnwukbt"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGI7A1dqunk7",
        "outputId": "bf7944f8-ff31-49b9-f69a-96d4e0c31959"
      },
      "source": [
        "# First 10 words from the list\n",
        "stopwords.words('english')[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-nUh1yVuq0Z"
      },
      "source": [
        "# Let us store them into a list and try to get rid of them.\n",
        "eng_stopwords = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1bL9pY27sQN"
      },
      "source": [
        "- Let us use that cleaned text to remove stopwords from this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3dQVLqvsvCkh",
        "outputId": "2df972a7-6b61-461f-8ad0-1f4209b74511"
      },
      "source": [
        "new_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hello how are you im very excited that youre going for a trip to europe yayy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sopauntv7ycH",
        "outputId": "8bd1884e-631e-4089-d89b-84e66e22f926"
      },
      "source": [
        "# Algorithm for removing stopwords.\n",
        "no_stopwords_text_list = []\n",
        "for i in new_text.split(' '):\n",
        "  if i in eng_stopwords:\n",
        "    pass\n",
        "  else:\n",
        "    no_stopwords_text_list.append(i)\n",
        "no_stopwords_text = ' '.join(no_stopwords_text_list)\n",
        "print(no_stopwords_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello im excited youre going trip europe yayy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OUVW4AW8U2d"
      },
      "source": [
        "- We can see the difference in the text. We got rid of alot noise from the text and by following this we can keep all the important words and can get rid of that noisy words.\n",
        "- Let us do some Stemming and Lemmatizing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CplkcNSR8ukF"
      },
      "source": [
        "## Stemming and Lemmatizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSW_Zcizc-xx"
      },
      "source": [
        "- **Stemming:**A technique that takes the word to its root form.\n",
        "- **Lemmatizing:**It also a technique to reduce a word to its root form.\n",
        "- But the only difference is that lemmatization will be very effective when used Parts of Speech(POS) Tagging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E7eIvEroY_Z"
      },
      "source": [
        "### Types of Stemming in NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCAEBJIFouJx"
      },
      "source": [
        "We basically have two types of Stemming in NLTK, there might be more but these two are very basic ones. They are- *Porter Stemmer, LancasterStemmer*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb0ripnp8TbR"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P-gbEGrpCjJ"
      },
      "source": [
        "Lets see the differences upon how differently they work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvyFHo7Mc9nF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e22b40c6-394b-4e14-ee7d-9415fd9656ce"
      },
      "source": [
        "#create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "#proide a word to be stemmed\n",
        "print(\"Stemming using Porter Stemmer:\")\n",
        "print('cats-->',porter.stem(\"cats\"))\n",
        "print('trouble-->',porter.stem(\"trouble\"))\n",
        "print('troubling-->',porter.stem(\"troubling\"))\n",
        "print('troubled-->',porter.stem(\"troubled\"))\n",
        "print('friendship-->',porter.stem(\"friendship\"))\n",
        "print('destabilize-->',porter.stem(\"destabilize\"))\n",
        "print(\"\\n\")\n",
        "print(\"Stemming using Lancaster Stemmer\")\n",
        "print('cats-->',lancaster.stem(\"cats\"))\n",
        "print('trouble-->',lancaster.stem(\"trouble\"))\n",
        "print('troubling-->',lancaster.stem(\"troubling\"))\n",
        "print('troubled-->',lancaster.stem(\"troubled\"))\n",
        "print('friendship-->',lancaster.stem(\"friendship\"))\n",
        "print('destabilize-->',lancaster.stem(\"destabilize\"))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemming using Porter Stemmer:\n",
            "cats--> cat\n",
            "trouble--> troubl\n",
            "troubling--> troubl\n",
            "troubled--> troubl\n",
            "friendship--> friendship\n",
            "destabilize--> destabil\n",
            "\n",
            "\n",
            "Stemming using Lancaster Stemmer\n",
            "cats--> cat\n",
            "trouble--> troubl\n",
            "troubling--> troubl\n",
            "troubled--> troubl\n",
            "friendship--> friend\n",
            "destabilize--> dest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trtt_7YyqScE"
      },
      "source": [
        "- We can see they dont differ alot within themselves but the problem is LancasterStemmer(Introduced in 1990) is very much aggressive while stemming than Porterstemmer(Developed in 1979). It takes time and is aggressive cause it iterates over each and every letter in a word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDHbcZZ6rf2N"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB_m5b9UrsdB"
      },
      "source": [
        "- Lemmatization basically is more convincing to apply cause it takes care of the context of the text and tries to make stemming more properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTj2n1N1te4Y",
        "outputId": "0aef20c4-48e7-415a-c653-f51172b8e67f"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zUh9sPqpGrR"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgsNhUrtsrK7"
      },
      "source": [
        "# Always lowercase words in your text and try to get rid of all those punctuations out of your text.\n",
        "\n",
        "sentence = \"he was running and eating at same time he has bad habit of swimming after playing long hours in the sun\""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__XC9EFjsuDO",
        "outputId": "c6794337-c6aa-4bbf-e1fe-55ac2412c660"
      },
      "source": [
        "print('Word before and after Lemmatizaion :')\n",
        "for words in sentence.split(' '):\n",
        "  print('{} --> {}'.format(words,wordnet_lemmatizer.lemmatize(words)))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word before and after Lemmatizaion :\n",
            "he --> he\n",
            "was --> wa\n",
            "running --> running\n",
            "and --> and\n",
            "eating --> eating\n",
            "at --> at\n",
            "same --> same\n",
            "time --> time\n",
            "he --> he\n",
            "has --> ha\n",
            "bad --> bad\n",
            "habit --> habit\n",
            "of --> of\n",
            "swimming --> swimming\n",
            "after --> after\n",
            "playing --> playing\n",
            "long --> long\n",
            "hours --> hour\n",
            "in --> in\n",
            "the --> the\n",
            "sun --> sun\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wc5A3mVuNOg"
      },
      "source": [
        "- You might be surprised by the results, But this is something that works by knowing the context of the text. So POS tagging makes it lot much better and actually makes it work.\n",
        "- In .lemmatize() method there is a parameter called as *'pos'* which accepts a single letter as its value and tags accordingly. In our case we set *'pos'='v'* cause mostly the verb forms of the words have extensions and we are worried about them to get rid of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nweIILWtaTJ",
        "outputId": "d8054f64-1b55-4416-9e22-4ec5910dff78"
      },
      "source": [
        "print('Word before and after Lemmatizaion using pos=v :')\n",
        "for words in sentence.split(' '):\n",
        "  print('{} --> {}'.format(words,wordnet_lemmatizer.lemmatize(words,pos='v')))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word before and after Lemmatizaion using pos=v :\n",
            "he --> he\n",
            "was --> be\n",
            "running --> run\n",
            "and --> and\n",
            "eating --> eat\n",
            "at --> at\n",
            "same --> same\n",
            "time --> time\n",
            "he --> he\n",
            "has --> have\n",
            "bad --> bad\n",
            "habit --> habit\n",
            "of --> of\n",
            "swimming --> swim\n",
            "after --> after\n",
            "playing --> play\n",
            "long --> long\n",
            "hours --> hours\n",
            "in --> in\n",
            "the --> the\n",
            "sun --> sun\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY6isRHgxuO_"
      },
      "source": [
        "- Now we see the difference. So the rule is to POS tag the sentence primarily and then apply Stemming i.e., Lemmatizing so that we stem the words properly based on the context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMa37OwzyhcO"
      },
      "source": [
        "- So this was all about the Stemming and all basic Preprocessing techniques."
      ]
    }
  ]
}